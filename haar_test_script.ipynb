{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# import pywt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "class HaarForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        super(HaarForward, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "\n",
    "        Arguments:\n",
    "            x (torch.Tensor): input tensor of shape [b, c, h, w]\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor of shape [b, c * 4, h / 2, w / 2]\n",
    "        \"\"\"\n",
    "\n",
    "        ll = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] + x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        lh = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] - x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hl = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] + x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hh = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] - x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        return torch.cat([ll,lh,hl,hh], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurDetectionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlurDetectionModel, self).__init__()\n",
    "        self.haar = HaarForward(0.5)\n",
    "        self.threshold = torch.tensor(35.0);\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # Assuming 'image' is a PyTorch tensor and 'threshold' is a float\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        while(len(image.shape)>2):\n",
    "            image = image.squeeze(0)\n",
    "        \n",
    "        blur_value1, blur_value2 = self.blur_detect(image)\n",
    "\n",
    "        # Convert the blur values back to PyTorch tensors\n",
    "        blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
    "        blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n",
    "        \n",
    "        return blur_tensor1, blur_tensor2\n",
    "    \n",
    "    def processHarrRes(self, _LL1):\n",
    "        LL1, LH1, HL1, HH1 = _LL1[:,0,:,:], _LL1[:,1,:,:], _LL1[:,2,:,:], _LL1[:,3,:,:]\n",
    "        LL1 = LL1.unsqueeze(0)\n",
    "        LH1 = LH1.squeeze(0)\n",
    "        HL1 = HL1.squeeze(0)\n",
    "        HH1 = HH1.squeeze(0)\n",
    "        return LL1, LH1, HL1, HH1\n",
    "\n",
    "    def blur_detect(self,Y):\n",
    "        M, N = Y.shape\n",
    "\n",
    "        # Crop input image to be 3 divisible by 2\n",
    "        Y = Y[0:int(M/16)*16, 0:int(N/16)*16]\n",
    "        # add batch dimension\n",
    "        Y = Y.unsqueeze(0).unsqueeze(0)\n",
    "        # Step 1, compute Haar wavelet of input image\n",
    "        _LL1 = self.haar(Y)\n",
    "        LL1, LH1, HL1, HH1 = self.processHarrRes(_LL1)\n",
    "        # Another application of 2D haar to LL1\n",
    "        _LL2= self.haar(LL1) \n",
    "        LL2, LH2, HL2, HH2 = self.processHarrRes(_LL2)\n",
    "        # Another application of 2D haar to LL2\n",
    "        _LL3 = self.haar(LL2)\n",
    "        LL3 , LH3, HL3, HH3 = self.processHarrRes(_LL3)\n",
    "        \n",
    "        LL1 = LL1.squeeze(0).squeeze(0)\n",
    "        LL2 = LL2.squeeze(0).squeeze(0)\n",
    "        LL3 = LL2.squeeze(0).squeeze(0)\n",
    "        # Construct the edge map in each scale Step 2\n",
    "        E1 = torch.sqrt(torch.pow(LH1, 2)+torch.pow(HL1, 2)+torch.pow(HH1, 2))\n",
    "        E2 = torch.sqrt(torch.pow(LH2, 2)+torch.pow(HL2, 2)+torch.pow(HH2, 2))\n",
    "        E3 = torch.sqrt(torch.pow(LH3, 2)+torch.pow(HL3, 2)+torch.pow(HH3, 2))\n",
    "        \n",
    "        M1, N1 = E1.shape\n",
    "\n",
    "\n",
    "        # Sliding window size level 1\n",
    "        sizeM1 = 8\n",
    "        sizeN1 = 8\n",
    "        \n",
    "        # Sliding windows size level 2\n",
    "        sizeM2 = int(sizeM1/2)\n",
    "        sizeN2 = int(sizeN1/2)\n",
    "        \n",
    "        # Sliding windows size level 3\n",
    "        sizeM3 = int(sizeM2/2)\n",
    "        sizeN3 = int(sizeN2/2)\n",
    "        \n",
    "        # Number of edge maps, related to sliding windows size\n",
    "        N_iter = int((M1/sizeM1)*(N1/sizeN1))\n",
    "        \n",
    "        Emax1 = torch.zeros((N_iter))\n",
    "        Emax2 = torch.zeros((N_iter))\n",
    "        Emax3 = torch.zeros((N_iter))\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Sliding windows index of level 1\n",
    "        x1 = 0\n",
    "        y1 = 0\n",
    "        # Sliding windows index of level 2\n",
    "        x2 = 0\n",
    "        y2 = 0\n",
    "        # Sliding windows index of level 3\n",
    "        x3 = 0\n",
    "        y3 = 0\n",
    "        \n",
    "        # Sliding windows limit on horizontal dimension\n",
    "        Y_limit = N1-sizeN1\n",
    "        \n",
    "        while count < N_iter:\n",
    "            # Get the maximum value of slicing windows over edge maps \n",
    "            # in each level\n",
    "            Emax1[count] = torch.max(E1[x1:x1+sizeM1,y1:y1+sizeN1])\n",
    "            Emax2[count] = torch.max(E2[x2:x2+sizeM2,y2:y2+sizeN2])\n",
    "            Emax3[count] = torch.max(E3[x3:x3+sizeM3,y3:y3+sizeN3])\n",
    "            \n",
    "            # if sliding windows ends horizontal direction\n",
    "            # move along vertical direction and resets horizontal\n",
    "            # direction\n",
    "            if y1 == Y_limit:\n",
    "                x1 = x1 + sizeM1\n",
    "                y1 = 0\n",
    "                \n",
    "                x2 = x2 + sizeM2\n",
    "                y2 = 0\n",
    "                \n",
    "                x3 = x3 + sizeM3\n",
    "                y3 = 0\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "            # windows moves along horizontal dimension\n",
    "            else:\n",
    "                    \n",
    "                y1 = y1 + sizeN1\n",
    "                y2 = y2 + sizeN2\n",
    "                y3 = y3 + sizeN3\n",
    "                count += 1\n",
    "        \n",
    "        # Step 3\n",
    "        EdgePoint1 = Emax1 > self.threshold;\n",
    "        EdgePoint2 = Emax2 > self.threshold;\n",
    "        EdgePoint3 = Emax3 > self.threshold;\n",
    "        \n",
    "        # Rule 1 Edge Pojnts\n",
    "        EdgePoint = EdgePoint1 + EdgePoint2 + EdgePoint3\n",
    "        \n",
    "        n_edges = EdgePoint.shape[0]\n",
    "        \n",
    "        # Rule 2 Dirak-Structure or Astep-Structure\n",
    "        DAstructure = (Emax1[EdgePoint] > Emax2[EdgePoint]) * (Emax2[EdgePoint] > Emax3[EdgePoint]);\n",
    "        \n",
    "        # Rule 3 Roof-Structure or Gstep-Structure\n",
    "        \n",
    "        RGstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < Emax2[i] and Emax2[i] < Emax3[i]:\n",
    "                \n",
    "                    RGstructure[i] = 1\n",
    "                    \n",
    "        # Rule 4 Roof-Structure\n",
    "        \n",
    "        RSstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax2[i] > Emax1[i] and Emax2[i] > Emax3[i]:\n",
    "                \n",
    "                    RSstructure[i] = 1\n",
    "\n",
    "        # Rule 5 Edge more likely to be in a blurred image \n",
    "\n",
    "        BlurC = torch.zeros((n_edges));\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if RGstructure[i] == 1 or RSstructure[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < self.threshold:\n",
    "                \n",
    "                    BlurC[i] = 1                        \n",
    "            \n",
    "        # Step 6\n",
    "        Per = torch.sum(DAstructure)/torch.sum(EdgePoint)\n",
    "        \n",
    "        # Step 7\n",
    "        if (torch.sum(RGstructure) + torch.sum(RSstructure)) == 0:\n",
    "            BlurExtent = torch.tensor(100)\n",
    "        else:\n",
    "            BlurExtent = torch.sum(BlurC) / (torch.sum(RGstructure) + torch.sum(RSstructure))\n",
    "        \n",
    "        return Per, BlurExtent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Example usage\n",
    "aspect_ratios = ['1:1', '4:3', '14:9', '16:10', '16:9', '37:20', '2:1', '21:9']                \n",
    "def get_resolution(aspect_ratio):\n",
    "    if(aspect_ratio == '1:1'):\n",
    "        return (600, 600)\n",
    "    elif(aspect_ratio == '4:3'):\n",
    "        return (800, 600)\n",
    "    elif(aspect_ratio == '14:9'):\n",
    "        return (1400, 900)\n",
    "    elif(aspect_ratio == '16:10'):\n",
    "        return (1280, 800)\n",
    "    elif(aspect_ratio == '16:9'):\n",
    "        return (1280, 720)\n",
    "    elif(aspect_ratio == '37:20'):\n",
    "        return (1850, 1000)\n",
    "    elif(aspect_ratio == '2:1'):\n",
    "        return (1200, 600)\n",
    "    elif(aspect_ratio == '21:9'):\n",
    "        return (2100, 900)\n",
    "    else:\n",
    "        return (1280, 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_images(input_dir):\n",
    "    extensions = [\".jpg\", \".png\", \".jpeg\"]\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1].lower() in extensions:\n",
    "                yield os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0\n",
      "images/blur/IMG_007097.jpg torch.Size([168, 300]) (tensor(0.0056), tensor(0.1012)) (tensor(0.0056), tensor(0.1012))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/2799566183.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
      "/var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/2799566183.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0\n",
      "images/blur/IMG_008987.jpg torch.Size([1080, 1920]) (tensor(0.0034), tensor(0.3800)) (tensor(0.0034), tensor(0.3800))\n",
      "255 0\n",
      "images/blur/IMG_035015.jpg torch.Size([1080, 1920]) (tensor(0.0034), tensor(0.3800)) (tensor(0.0034), tensor(0.3800))\n",
      "255 0\n",
      "images/blur/IMG_036905.jpg torch.Size([168, 300]) (tensor(0.0056), tensor(0.1012)) (tensor(0.0056), tensor(0.1012))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of the custom model\n",
    "model = BlurDetectionModel()\n",
    "# for ar in aspect_ratios:\n",
    "# input_image = torch.randn(1080, 720)  # Replace with your actual image dimensions\n",
    "torchtraced_model = torch.jit.script(model)\n",
    "# print('diff', traced_out[0] - blur_result1, traced_out[1] - blur_result2)\n",
    "imageFiles = ['images/blur/IMG_007097.jpg', 'images/blur/IMG_008987.jpg', 'images/blur/IMG_035015.jpg', 'images/blur/IMG_036905.jpg']\n",
    "res = find_images('images')\n",
    "\n",
    "for path in imageFiles:\n",
    "    img = cv2.imread(path)\n",
    "    Y = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    print(max(Y.flatten()), min(Y.flatten()))\n",
    "    input_image = torch.tensor(Y, dtype=torch.float32)\n",
    "    blur_result = model(input_image)\n",
    "    traced_out = torchtraced_model(input_image)\n",
    "\n",
    "    print(path, input_image.shape, traced_out , blur_result)\n",
    "\n",
    "torch.jit.save(torchtraced_model, 'haar_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self : __torch__.___torch_mangle_25.BlurDetectionModel,\n",
       "      %image.1 : Tensor):\n",
       "  %22 : bool = prim::Constant[value=0]()\n",
       "  %21 : NoneType = prim::Constant()\n",
       "  %19 : int = prim::Constant[value=6]() # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:16:55\n",
       "  %2 : int = prim::Constant[value=9223372036854775807]() # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:8\n",
       "  %6 : int = prim::Constant[value=2]() # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:31\n",
       "  %11 : int = prim::Constant[value=0]() # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:11:34\n",
       "  %40 : int[] = aten::size(%image.1) # <string>:13:9\n",
       "  %41 : int = aten::len(%40) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:14\n",
       "  %42 : bool = aten::gt(%41, %6) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:14\n",
       "  %image : Tensor = prim::Loop(%2, %42, %image.1) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:8\n",
       "    block0(%9 : int, %image.13 : Tensor):\n",
       "      %image.7 : Tensor = aten::squeeze(%image.13, %11) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:11:20\n",
       "      %4 : int[] = aten::size(%image.7) # <string>:13:9\n",
       "      %5 : int = aten::len(%4) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:14\n",
       "      %7 : bool = aten::gt(%5, %6) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:10:14\n",
       "      -> (%7, %image.7)\n",
       "  %15 : (Tensor, Tensor) = prim::CallMethod[name=\"blur_detect\"](%self, %image) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:13:35\n",
       "  %blur_value1.1 : Tensor, %blur_value2.1 : Tensor = prim::TupleUnpack(%15)\n",
       "  %20 : float = aten::FloatImplicit(%blur_value1.1) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:16:23\n",
       "  %blur_tensor1.1 : Tensor = aten::tensor(%20, %19, %21, %22) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:16:23\n",
       "  %26 : float = aten::FloatImplicit(%blur_value2.1) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:17:23\n",
       "  %blur_tensor2.1 : Tensor = aten::tensor(%26, %19, %21, %22) # /var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_5011/3398226446.py:17:23\n",
       "  %32 : Tensor[] = prim::ListConstruct(%blur_tensor1.1, %blur_tensor2.1)\n",
       "  return (%32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtraced_model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfpgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
