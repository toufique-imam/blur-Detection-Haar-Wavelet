{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_39825/1307287381.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
      "/var/folders/bq/x_1p9f1n4qnddvzfr8kjb59h0000gn/T/ipykernel_39825/1307287381.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff tensor(0.) tensor(0.)\n",
      "diff tensor(0.) tensor(0.)\n",
      "diff tensor(0.) tensor(0.)\n",
      "diff tensor(0.) tensor(0.)\n",
      "diff tensor(0.) tensor(0.)\n",
      "diff tensor(0.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# import pywt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "class HaarForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        super(HaarForward, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "\n",
    "        Arguments:\n",
    "            x (torch.Tensor): input tensor of shape [b, c, h, w]\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor of shape [b, c * 4, h / 2, w / 2]\n",
    "        \"\"\"\n",
    "\n",
    "        ll = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] + x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        lh = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] - x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hl = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] + x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hh = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] - x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        return torch.cat([ll,lh,hl,hh], 1)\n",
    "\n",
    "class BlurDetectionModel(torch.nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(BlurDetectionModel, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.haar = HaarForward(0.5)\n",
    "\n",
    "    def forward(self, image):\n",
    "        # Assuming 'image' is a PyTorch tensor and 'threshold' is a float\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "\n",
    "        blur_value1, blur_value2 = self.blur_detect(image, self.threshold)\n",
    "\n",
    "        # Convert the blur values back to PyTorch tensors\n",
    "        blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
    "        blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n",
    "\n",
    "        return blur_tensor1, blur_tensor2\n",
    "    \n",
    "    def processHarrRes(self, _LL1):\n",
    "        LL1, LH1, HL1, HH1 = _LL1[:,0,:,:], _LL1[:,1,:,:], _LL1[:,2,:,:], _LL1[:,3,:,:]\n",
    "        LL1 = LL1.unsqueeze(0)\n",
    "        LH1 = LH1.squeeze(0)\n",
    "        HL1 = HL1.squeeze(0)\n",
    "        HH1 = HH1.squeeze(0)\n",
    "        return LL1, LH1, HL1, HH1\n",
    "\n",
    "    def blur_detect(self,Y, threshold):\n",
    "        M, N = Y.shape\n",
    "\n",
    "        # Crop input image to be 3 divisible by 2\n",
    "        Y = Y[0:int(M/16)*16, 0:int(N/16)*16]\n",
    "        # add batch dimension\n",
    "        Y = Y.unsqueeze(0).unsqueeze(0)\n",
    "        # Step 1, compute Haar wavelet of input image\n",
    "        _LL1 = self.haar(Y)\n",
    "        LL1, LH1, HL1, HH1 = self.processHarrRes(_LL1)\n",
    "        # Another application of 2D haar to LL1\n",
    "        _LL2= self.haar(LL1) \n",
    "        LL2, LH2, HL2, HH2 = self.processHarrRes(_LL2)\n",
    "        # Another application of 2D haar to LL2\n",
    "        _LL3 = self.haar(LL2)\n",
    "        LL3 , LH3, HL3, HH3 = self.processHarrRes(_LL3)\n",
    "        \n",
    "        LL1 = LL1.squeeze(0).squeeze(0)\n",
    "        LL2 = LL2.squeeze(0).squeeze(0)\n",
    "        LL3 = LL2.squeeze(0).squeeze(0)\n",
    "        # Construct the edge map in each scale Step 2\n",
    "        E1 = torch.sqrt(torch.pow(LH1, 2)+torch.pow(HL1, 2)+torch.pow(HH1, 2))\n",
    "        E2 = torch.sqrt(torch.pow(LH2, 2)+torch.pow(HL2, 2)+torch.pow(HH2, 2))\n",
    "        E3 = torch.sqrt(torch.pow(LH3, 2)+torch.pow(HL3, 2)+torch.pow(HH3, 2))\n",
    "        \n",
    "        M1, N1 = E1.shape\n",
    "\n",
    "\n",
    "        # Sliding window size level 1\n",
    "        sizeM1 = 8\n",
    "        sizeN1 = 8\n",
    "        \n",
    "        # Sliding windows size level 2\n",
    "        sizeM2 = int(sizeM1/2)\n",
    "        sizeN2 = int(sizeN1/2)\n",
    "        \n",
    "        # Sliding windows size level 3\n",
    "        sizeM3 = int(sizeM2/2)\n",
    "        sizeN3 = int(sizeN2/2)\n",
    "        \n",
    "        # Number of edge maps, related to sliding windows size\n",
    "        N_iter = int((M1/sizeM1)*(N1/sizeN1))\n",
    "        \n",
    "        Emax1 = torch.zeros((N_iter))\n",
    "        Emax2 = torch.zeros((N_iter))\n",
    "        Emax3 = torch.zeros((N_iter))\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Sliding windows index of level 1\n",
    "        x1 = 0\n",
    "        y1 = 0\n",
    "        # Sliding windows index of level 2\n",
    "        x2 = 0\n",
    "        y2 = 0\n",
    "        # Sliding windows index of level 3\n",
    "        x3 = 0\n",
    "        y3 = 0\n",
    "        \n",
    "        # Sliding windows limit on horizontal dimension\n",
    "        Y_limit = N1-sizeN1\n",
    "        \n",
    "        while count < N_iter:\n",
    "            # Get the maximum value of slicing windows over edge maps \n",
    "            # in each level\n",
    "            Emax1[count] = torch.max(E1[x1:x1+sizeM1,y1:y1+sizeN1])\n",
    "            Emax2[count] = torch.max(E2[x2:x2+sizeM2,y2:y2+sizeN2])\n",
    "            Emax3[count] = torch.max(E3[x3:x3+sizeM3,y3:y3+sizeN3])\n",
    "            \n",
    "            # if sliding windows ends horizontal direction\n",
    "            # move along vertical direction and resets horizontal\n",
    "            # direction\n",
    "            if y1 == Y_limit:\n",
    "                x1 = x1 + sizeM1\n",
    "                y1 = 0\n",
    "                \n",
    "                x2 = x2 + sizeM2\n",
    "                y2 = 0\n",
    "                \n",
    "                x3 = x3 + sizeM3\n",
    "                y3 = 0\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "            # windows moves along horizontal dimension\n",
    "            else:\n",
    "                    \n",
    "                y1 = y1 + sizeN1\n",
    "                y2 = y2 + sizeN2\n",
    "                y3 = y3 + sizeN3\n",
    "                count += 1\n",
    "        \n",
    "        # Step 3\n",
    "        EdgePoint1 = Emax1 > threshold;\n",
    "        EdgePoint2 = Emax2 > threshold;\n",
    "        EdgePoint3 = Emax3 > threshold;\n",
    "        \n",
    "        # Rule 1 Edge Pojnts\n",
    "        EdgePoint = EdgePoint1 + EdgePoint2 + EdgePoint3\n",
    "        \n",
    "        n_edges = EdgePoint.shape[0]\n",
    "        \n",
    "        # Rule 2 Dirak-Structure or Astep-Structure\n",
    "        DAstructure = (Emax1[EdgePoint] > Emax2[EdgePoint]) * (Emax2[EdgePoint] > Emax3[EdgePoint]);\n",
    "        \n",
    "        # Rule 3 Roof-Structure or Gstep-Structure\n",
    "        \n",
    "        RGstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < Emax2[i] and Emax2[i] < Emax3[i]:\n",
    "                \n",
    "                    RGstructure[i] = 1\n",
    "                    \n",
    "        # Rule 4 Roof-Structure\n",
    "        \n",
    "        RSstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax2[i] > Emax1[i] and Emax2[i] > Emax3[i]:\n",
    "                \n",
    "                    RSstructure[i] = 1\n",
    "\n",
    "        # Rule 5 Edge more likely to be in a blurred image \n",
    "\n",
    "        BlurC = torch.zeros((n_edges));\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if RGstructure[i] == 1 or RSstructure[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < threshold:\n",
    "                \n",
    "                    BlurC[i] = 1                        \n",
    "            \n",
    "        # Step 6\n",
    "        Per = torch.sum(DAstructure)/torch.sum(EdgePoint)\n",
    "        \n",
    "        # Step 7\n",
    "        if (torch.sum(RGstructure) + torch.sum(RSstructure)) == 0:\n",
    "            \n",
    "            BlurExtent = torch.tensor(100)\n",
    "        else:\n",
    "            BlurExtent = torch.sum(BlurC) / (torch.sum(RGstructure) + torch.sum(RSstructure))\n",
    "        \n",
    "        return Per, BlurExtent\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the custom model\n",
    "model = BlurDetectionModel(torch.tensor(0.2))\n",
    "\n",
    "import time\n",
    "# Example usage\n",
    "aspect_ratios = ['1:1', '4:3', '14:9', '16:10', '16:9', '37:20', '2:1', '21:9']                \n",
    "def get_resolution(aspect_ratio):\n",
    "    if(aspect_ratio == '1:1'):\n",
    "        return (600, 600)\n",
    "    elif(aspect_ratio == '4:3'):\n",
    "        return (800, 600)\n",
    "    elif(aspect_ratio == '14:9'):\n",
    "        return (1400, 900)\n",
    "    elif(aspect_ratio == '16:10'):\n",
    "        return (1280, 800)\n",
    "    elif(aspect_ratio == '16:9'):\n",
    "        return (1280, 720)\n",
    "    elif(aspect_ratio == '37:20'):\n",
    "        return (1850, 1000)\n",
    "    elif(aspect_ratio == '2:1'):\n",
    "        return (1200, 600)\n",
    "    elif(aspect_ratio == '21:9'):\n",
    "        return (2100, 900)\n",
    "    else:\n",
    "        return (1280, 720)\n",
    "    \n",
    "_st = time.time()\n",
    "\n",
    "# for ar in aspect_ratios:\n",
    "res = get_resolution(aspect_ratios[0])\n",
    "input_image = torch.randn(res[0], res[1])  # Replace with your actual image dimensions\n",
    "# blur_threshold = 0.5  # Replace with your desired threshold\n",
    "print(res)\n",
    "blur_result1, blur_result2 = model(input_image)\n",
    "# input_image = torch.randn(1080, 720)  # Replace with your actual image dimensions\n",
    "torchtraced_model = torch.jit.script(model)\n",
    "traced_out = torchtraced_model(input_image)\n",
    "print('diff', traced_out[0] - blur_result1, traced_out[1] - blur_result2)\n",
    "for i in range(5):\n",
    "    input_image = torch.randn(get_resolution(aspect_ratios[1]))  # Replace with your actual image dimensions\n",
    "    blur_result1, blur_result2 = model(input_image)\n",
    "    traced_out = torchtraced_model(input_image)\n",
    "\n",
    "    print('diff', traced_out[0] - blur_result1, traced_out[1] - blur_result2)\n",
    "\n",
    "torch.jit.save(torchtraced_model, 'haar_scripted_'+str(res[0])+'_'+str(res[1])+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfpgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
