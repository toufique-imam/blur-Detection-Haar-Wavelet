{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# import pywt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "class HaarForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        super(HaarForward, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a 2d DWT Forward decomposition of an image using Haar Wavelets\n",
    "\n",
    "        Arguments:\n",
    "            x (torch.Tensor): input tensor of shape [b, c, h, w]\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor of shape [b, c * 4, h / 2, w / 2]\n",
    "        \"\"\"\n",
    "\n",
    "        ll = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] + x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        lh = self.alpha * (x[:,:,0::2,0::2] + x[:,:,0::2,1::2] - x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hl = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] + x[:,:,1::2,0::2] - x[:,:,1::2,1::2])\n",
    "        hh = self.alpha * (x[:,:,0::2,0::2] - x[:,:,0::2,1::2] - x[:,:,1::2,0::2] + x[:,:,1::2,1::2])\n",
    "        return torch.cat([ll,lh,hl,hh], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurDetectionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlurDetectionModel, self).__init__()\n",
    "        self.haar = HaarForward(0.5)\n",
    "        self.threshold = torch.tensor(35.0);\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # Assuming 'image' is a PyTorch tensor and 'threshold' is a float\n",
    "        # Convert the PyTorch tensor to a NumPy array\n",
    "        \n",
    "        blur_value1, blur_value2 = self.blur_detect(image)\n",
    "        # max_brightness = torch.max(image)\n",
    "        # min_brightness = torch.max(torch.min(image), torch.tensor(10.0))\n",
    "        # print(max_brightness, min_brightness)\n",
    "        \n",
    "        # dynamic_range = torch.round(torch.log2(max_brightness) - torch.log2(min_brightness), decimals=2)\n",
    "        # Convert the blur values back to PyTorch tensors\n",
    "        blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
    "        blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n",
    "        # dynamic_range_tensor = torch.tensor(dynamic_range, dtype=torch.float32)\n",
    "        \n",
    "        return blur_tensor1, blur_tensor2 #, dynamic_range_tensor\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def processHarrRes(self, _LL1):\n",
    "        LL1, LH1, HL1, HH1 = _LL1[:,0,:,:], _LL1[:,1,:,:], _LL1[:,2,:,:], _LL1[:,3,:,:]\n",
    "        LL1 = LL1.unsqueeze(0)\n",
    "        LH1 = LH1.squeeze(0)\n",
    "        HL1 = HL1.squeeze(0)\n",
    "        HH1 = HH1.squeeze(0)\n",
    "        return LL1, LH1, HL1, HH1\n",
    "\n",
    "    def blur_detect(self,Y):\n",
    "        M, N = Y.shape\n",
    "\n",
    "        # Crop input image to be 3 divisible by 2\n",
    "        Y = Y[0:int(M/16)*16, 0:int(N/16)*16]\n",
    "        # add batch dimension\n",
    "        Y = Y.unsqueeze(0).unsqueeze(0)\n",
    "        # Step 1, compute Haar wavelet of input image\n",
    "        _LL1 = self.haar(Y)\n",
    "        LL1, LH1, HL1, HH1 = self.processHarrRes(_LL1)\n",
    "        # Another application of 2D haar to LL1\n",
    "        _LL2= self.haar(LL1) \n",
    "        LL2, LH2, HL2, HH2 = self.processHarrRes(_LL2)\n",
    "        # Another application of 2D haar to LL2\n",
    "        _LL3 = self.haar(LL2)\n",
    "        LL3 , LH3, HL3, HH3 = self.processHarrRes(_LL3)\n",
    "        \n",
    "        LL1 = LL1.squeeze(0).squeeze(0)\n",
    "        LL2 = LL2.squeeze(0).squeeze(0)\n",
    "        LL3 = LL2.squeeze(0).squeeze(0)\n",
    "        # Construct the edge map in each scale Step 2\n",
    "        E1 = torch.sqrt(torch.pow(LH1, 2)+torch.pow(HL1, 2)+torch.pow(HH1, 2))\n",
    "        E2 = torch.sqrt(torch.pow(LH2, 2)+torch.pow(HL2, 2)+torch.pow(HH2, 2))\n",
    "        E3 = torch.sqrt(torch.pow(LH3, 2)+torch.pow(HL3, 2)+torch.pow(HH3, 2))\n",
    "        \n",
    "        M1, N1 = E1.shape\n",
    "\n",
    "\n",
    "        # Sliding window size level 1\n",
    "        sizeM1 = 8\n",
    "        sizeN1 = 8\n",
    "        \n",
    "        # Sliding windows size level 2\n",
    "        sizeM2 = int(sizeM1/2)\n",
    "        sizeN2 = int(sizeN1/2)\n",
    "        \n",
    "        # Sliding windows size level 3\n",
    "        sizeM3 = int(sizeM2/2)\n",
    "        sizeN3 = int(sizeN2/2)\n",
    "        \n",
    "        # Number of edge maps, related to sliding windows size\n",
    "        N_iter = int((M1/sizeM1)*(N1/sizeN1))\n",
    "        \n",
    "        Emax1 = torch.zeros((N_iter))\n",
    "        Emax2 = torch.zeros((N_iter))\n",
    "        Emax3 = torch.zeros((N_iter))\n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        # Sliding windows index of level 1\n",
    "        x1 = 0\n",
    "        y1 = 0\n",
    "        # Sliding windows index of level 2\n",
    "        x2 = 0\n",
    "        y2 = 0\n",
    "        # Sliding windows index of level 3\n",
    "        x3 = 0\n",
    "        y3 = 0\n",
    "        \n",
    "        # Sliding windows limit on horizontal dimension\n",
    "        Y_limit = N1-sizeN1\n",
    "        \n",
    "        while count < N_iter:\n",
    "            # Get the maximum value of slicing windows over edge maps \n",
    "            # in each level\n",
    "            Emax1[count] = torch.max(E1[x1:x1+sizeM1,y1:y1+sizeN1])\n",
    "            Emax2[count] = torch.max(E2[x2:x2+sizeM2,y2:y2+sizeN2])\n",
    "            Emax3[count] = torch.max(E3[x3:x3+sizeM3,y3:y3+sizeN3])\n",
    "            \n",
    "            # if sliding windows ends horizontal direction\n",
    "            # move along vertical direction and resets horizontal\n",
    "            # direction\n",
    "            if y1 == Y_limit:\n",
    "                x1 = x1 + sizeM1\n",
    "                y1 = 0\n",
    "                \n",
    "                x2 = x2 + sizeM2\n",
    "                y2 = 0\n",
    "                \n",
    "                x3 = x3 + sizeM3\n",
    "                y3 = 0\n",
    "                \n",
    "                count += 1\n",
    "            \n",
    "            # windows moves along horizontal dimension\n",
    "            else:\n",
    "                    \n",
    "                y1 = y1 + sizeN1\n",
    "                y2 = y2 + sizeN2\n",
    "                y3 = y3 + sizeN3\n",
    "                count += 1\n",
    "        \n",
    "        # Step 3\n",
    "        EdgePoint1 = Emax1 > self.threshold;\n",
    "        EdgePoint2 = Emax2 > self.threshold;\n",
    "        EdgePoint3 = Emax3 > self.threshold;\n",
    "        \n",
    "        # Rule 1 Edge Pojnts\n",
    "        EdgePoint = EdgePoint1 + EdgePoint2 + EdgePoint3\n",
    "        \n",
    "        n_edges = EdgePoint.shape[0]\n",
    "        \n",
    "        # Rule 2 Dirak-Structure or Astep-Structure\n",
    "        DAstructure = (Emax1[EdgePoint] > Emax2[EdgePoint]) * (Emax2[EdgePoint] > Emax3[EdgePoint]);\n",
    "        \n",
    "        # Rule 3 Roof-Structure or Gstep-Structure\n",
    "        \n",
    "        RGstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < Emax2[i] and Emax2[i] < Emax3[i]:\n",
    "                \n",
    "                    RGstructure[i] = 1\n",
    "                    \n",
    "        # Rule 4 Roof-Structure\n",
    "        \n",
    "        RSstructure = torch.zeros((n_edges))\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if EdgePoint[i] == 1:\n",
    "            \n",
    "                if Emax2[i] > Emax1[i] and Emax2[i] > Emax3[i]:\n",
    "                \n",
    "                    RSstructure[i] = 1\n",
    "\n",
    "        # Rule 5 Edge more likely to be in a blurred image \n",
    "\n",
    "        BlurC = torch.zeros((n_edges));\n",
    "\n",
    "        for i in range(n_edges):\n",
    "        \n",
    "            if RGstructure[i] == 1 or RSstructure[i] == 1:\n",
    "            \n",
    "                if Emax1[i] < self.threshold:\n",
    "                \n",
    "                    BlurC[i] = 1                        \n",
    "            \n",
    "        # Step 6\n",
    "        Per = torch.sum(DAstructure)/torch.sum(EdgePoint)\n",
    "        \n",
    "        # Step 7\n",
    "        if (torch.sum(RGstructure) + torch.sum(RSstructure)) == 0:\n",
    "            BlurExtent = torch.tensor(100)\n",
    "        else:\n",
    "            BlurExtent = torch.sum(BlurC) / (torch.sum(RGstructure) + torch.sum(RSstructure))\n",
    "        \n",
    "        return Per, BlurExtent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Example usage\n",
    "aspect_ratios = ['1:1', '4:3', '14:9', '16:10', '16:9', '37:20', '2:1', '21:9']                \n",
    "def get_resolution(aspect_ratio):\n",
    "    if(aspect_ratio == '1:1'):\n",
    "        return (600, 600)\n",
    "    elif(aspect_ratio == '4:3'):\n",
    "        return (800, 600)\n",
    "    elif(aspect_ratio == '14:9'):\n",
    "        return (1400, 900)\n",
    "    elif(aspect_ratio == '16:10'):\n",
    "        return (1280, 800)\n",
    "    elif(aspect_ratio == '16:9'):\n",
    "        return (1280, 720)\n",
    "    elif(aspect_ratio == '37:20'):\n",
    "        return (1850, 1000)\n",
    "    elif(aspect_ratio == '2:1'):\n",
    "        return (1200, 600)\n",
    "    elif(aspect_ratio == '21:9'):\n",
    "        return (2100, 900)\n",
    "    else:\n",
    "        return (1280, 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_images(input_dir):\n",
    "    extensions = [\".jpg\", \".png\", \".jpeg\"]\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if os.path.splitext(file)[1].lower() in extensions:\n",
    "                yield os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/zmpgwhys09743q0lqh1rh65m0000gn/T/ipykernel_20884/3427172537.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor1 = torch.tensor(blur_value1, dtype=torch.float32)\n",
      "/var/folders/fq/zmpgwhys09743q0lqh1rh65m0000gn/T/ipykernel_20884/3427172537.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  blur_tensor2 = torch.tensor(blur_value2, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/hdr_test_case/4.jpeg (tensor(0.0036), tensor(0.4842)) (tensor(0.0036), tensor(0.4842))\n",
      "images/hdr_test_case/5.jpeg (tensor(0.0068), tensor(0.3868)) (tensor(0.0068), tensor(0.3868))\n",
      "images/hdr_test_case/2.jpg (tensor(0.0075), tensor(0.2967)) (tensor(0.0075), tensor(0.2967))\n",
      "images/hdr_test_case/3.jpg (tensor(0.0102), tensor(0.3480)) (tensor(0.0102), tensor(0.3480))\n",
      "images/hdr_test_case/1.jpg (tensor(0.0119), tensor(0.3380)) (tensor(0.0119), tensor(0.3380))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of the custom model\n",
    "model = BlurDetectionModel()\n",
    "# model.eval()\n",
    "model.training = False\n",
    "# for ar in aspect_ratios:\n",
    "# input_image = torch.randn(1080, 720)  # Replace with your actual image dimensions\n",
    "torchtraced_model = torch.jit.script(model)\n",
    "# torchtraced_model.eval()\n",
    "# print('diff', traced_out[0] - blur_result1, traced_out[1] - blur_result2)\n",
    "imageFiles = ['images/blur/IMG_007097.jpg', 'images/blur/IMG_008987.jpg']\n",
    "res = find_images('images/hdr_test_case/')\n",
    "\n",
    "for path in res:\n",
    "    img = cv2.imread(path)\n",
    "    Y = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)[:,:,0]\n",
    "    # _x = Y\n",
    "    # print(_x.shape)\n",
    "    # cnt = 0\n",
    "    # for i in range(0, _x.shape[0]):\n",
    "        # for j in range(0, _x.shape[1]):\n",
    "            # if(_x[i,j] < 1):\n",
    "                # cnt = cnt + 1;    \n",
    "    # print(cnt,  max(_x.flatten()), min(_x.flatten()))\n",
    "    \n",
    "    input_image = torch.tensor(Y, dtype=torch.float32)\n",
    "    blur_result = model(input_image)\n",
    "    traced_out = torchtraced_model(input_image)\n",
    "\n",
    "    print(path, traced_out , blur_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aten::Bool.Tensor', 'aten::FloatImplicit', 'aten::Int.float', 'aten::__getitem__.t', 'aten::add.Tensor', 'aten::add.int', 'aten::cat', 'aten::copy_', 'aten::div.Tensor', 'aten::div.int', 'aten::eq.Scalar', 'aten::eq.int', 'aten::gt.Tensor', 'aten::index.Tensor', 'aten::lt.Tensor', 'aten::lt.int', 'aten::max', 'aten::mul.Scalar', 'aten::mul.Tensor', 'aten::mul.float', 'aten::mul.int', 'aten::pow.Tensor_Scalar', 'aten::select.int', 'aten::size', 'aten::slice.Tensor', 'aten::sqrt', 'aten::squeeze.dim', 'aten::sub.Tensor', 'aten::sub.int', 'aten::sum', 'aten::tensor.float', 'aten::tensor.int', 'aten::unsqueeze', 'aten::zeros']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "torchtraced_model.to('mps')\n",
    "optimized_model = optimize_for_mobile(torchtraced_model, backend='Metal')\n",
    "# optimized_model.training = False\n",
    "# optimized_model.eval()\n",
    "print(torch.jit.export_opnames(optimized_model))\n",
    "optimized_model._save_for_lite_interpreter('haar_scripted_metal.pth')\n",
    "torch.jit.save(optimized_model, 'haar_scripted_opt.pt')\n",
    "torch.jit.save(torchtraced_model, 'haar_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=BlurDetectionModel)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Support for converting Torch Script Models is experimental. If possible you should use a traced model for conversion.\n",
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  11%|â–ˆ         | 46/415 [00:00<00:00, 7815.05 ops/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Torch var alpha.2 not found in context ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcoremltools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mct\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m coreml_model \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m      4\u001b[0m     torchtraced_model, convert_to\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmlprogram\u001b[39;49m\u001b[39m'\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m[ct\u001b[39m.\u001b[39;49mTensorType(shape\u001b[39m=\u001b[39;49m(\u001b[39m600\u001b[39;49m, \u001b[39m600\u001b[39;49m))])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/_converters_entry.py:492\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m specification_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     specification_version \u001b[39m=\u001b[39m _set_default_specification_version(exact_target)\n\u001b[0;32m--> 492\u001b[0m mlmodel \u001b[39m=\u001b[39m mil_convert(\n\u001b[1;32m    493\u001b[0m     model,\n\u001b[1;32m    494\u001b[0m     convert_from\u001b[39m=\u001b[39;49mexact_source,\n\u001b[1;32m    495\u001b[0m     convert_to\u001b[39m=\u001b[39;49mexact_target,\n\u001b[1;32m    496\u001b[0m     inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    497\u001b[0m     outputs\u001b[39m=\u001b[39;49moutputs_as_tensor_or_image_types,  \u001b[39m# None or list[ct.ImageType/ct.TensorType]\u001b[39;49;00m\n\u001b[1;32m    498\u001b[0m     classifier_config\u001b[39m=\u001b[39;49mclassifier_config,\n\u001b[1;32m    499\u001b[0m     skip_model_load\u001b[39m=\u001b[39;49mskip_model_load,\n\u001b[1;32m    500\u001b[0m     compute_units\u001b[39m=\u001b[39;49mcompute_units,\n\u001b[1;32m    501\u001b[0m     package_dir\u001b[39m=\u001b[39;49mpackage_dir,\n\u001b[1;32m    502\u001b[0m     debug\u001b[39m=\u001b[39;49mdebug,\n\u001b[1;32m    503\u001b[0m     specification_version\u001b[39m=\u001b[39;49mspecification_version,\n\u001b[1;32m    504\u001b[0m     main_pipeline\u001b[39m=\u001b[39;49mpass_pipeline,\n\u001b[1;32m    505\u001b[0m )\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m exact_target \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmilinternal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mreturn\u001b[39;00m mlmodel  \u001b[39m# Returns the MIL program\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:188\u001b[0m, in \u001b[0;36mmil_convert\u001b[0;34m(model, convert_from, convert_to, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m@_profile\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmil_convert\u001b[39m(\n\u001b[1;32m    151\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    156\u001b[0m ):\n\u001b[1;32m    157\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    Convert model from a specified frontend `convert_from` to a specified\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m    converter backend `convert_to`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m        See `coremltools.converters.convert`\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[39mreturn\u001b[39;00m _mil_convert(model, convert_from, convert_to, ConverterRegistry, MLModel, compute_units, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:212\u001b[0m, in \u001b[0;36m_mil_convert\u001b[0;34m(model, convert_from, convert_to, registry, modelClass, compute_units, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     weights_dir \u001b[39m=\u001b[39m _tempfile\u001b[39m.\u001b[39mTemporaryDirectory()\n\u001b[1;32m    210\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mweights_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m weights_dir\u001b[39m.\u001b[39mname\n\u001b[0;32m--> 212\u001b[0m proto, mil_program \u001b[39m=\u001b[39m mil_convert_to_proto(\n\u001b[1;32m    213\u001b[0m                         model,\n\u001b[1;32m    214\u001b[0m                         convert_from,\n\u001b[1;32m    215\u001b[0m                         convert_to,\n\u001b[1;32m    216\u001b[0m                         registry,\n\u001b[1;32m    217\u001b[0m                         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    218\u001b[0m                      )\n\u001b[1;32m    220\u001b[0m _reset_conversion_state()\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m convert_to \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmilinternal\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:285\u001b[0m, in \u001b[0;36mmil_convert_to_proto\u001b[0;34m(model, convert_from, convert_to, converter_registry, main_pipeline, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m frontend_pipeline, backend_pipeline \u001b[39m=\u001b[39m _construct_other_pipelines(\n\u001b[1;32m    281\u001b[0m     main_pipeline, convert_from, convert_to\n\u001b[1;32m    282\u001b[0m )\n\u001b[1;32m    284\u001b[0m frontend_converter \u001b[39m=\u001b[39m frontend_converter_type()\n\u001b[0;32m--> 285\u001b[0m prog \u001b[39m=\u001b[39m frontend_converter(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    286\u001b[0m PipelineManager\u001b[39m.\u001b[39mapply_pipeline(prog, frontend_pipeline)\n\u001b[1;32m    288\u001b[0m PipelineManager\u001b[39m.\u001b[39mapply_pipeline(prog, main_pipeline)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/converter.py:108\u001b[0m, in \u001b[0;36mTorchFrontend.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    106\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfrontend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m load(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/load.py:63\u001b[0m, in \u001b[0;36mload\u001b[0;34m(model_spec, inputs, specification_version, debug, outputs, cut_at_symbols, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m inputs \u001b[39m=\u001b[39m _convert_to_torch_inputtype(inputs)\n\u001b[1;32m     56\u001b[0m converter \u001b[39m=\u001b[39m TorchConverter(\n\u001b[1;32m     57\u001b[0m     torchscript,\n\u001b[1;32m     58\u001b[0m     inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     specification_version,\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m _perform_torch_convert(converter, debug)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/load.py:102\u001b[0m, in \u001b[0;36m_perform_torch_convert\u001b[0;34m(converter, debug)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_perform_torch_convert\u001b[39m(converter, debug):\n\u001b[1;32m    101\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         prog \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m    103\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m         \u001b[39mif\u001b[39;00m debug \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconvert function\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/converter.py:284\u001b[0m, in \u001b[0;36mTorchConverter.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_const()\n\u001b[1;32m    283\u001b[0m \u001b[39m# Add the rest of the operations\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m convert_nodes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph)\n\u001b[1;32m    286\u001b[0m graph_outputs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext[name] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39moutputs]\n\u001b[1;32m    288\u001b[0m \u001b[39m# An output can be None when it's a None constant, which happens\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# in Fairseq MT.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/ops.py:88\u001b[0m, in \u001b[0;36mconvert_nodes\u001b[0;34m(context, graph)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPyTorch convert function for op \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not implemented.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(node\u001b[39m.\u001b[39mkind)\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     87\u001b[0m context\u001b[39m.\u001b[39mprepare_for_conversion(node)\n\u001b[0;32m---> 88\u001b[0m add_op(context, node)\n\u001b[1;32m     90\u001b[0m \u001b[39m# We've generated all the outputs the graph needs, terminate conversion.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m _all_outputs_present(context, graph):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/ops.py:1393\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(context, node)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[39m@register_torch_op\u001b[39m\n\u001b[1;32m   1392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(context, node):\n\u001b[0;32m-> 1393\u001b[0m     inputs \u001b[39m=\u001b[39m _get_inputs(context, node, expected\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m   1394\u001b[0m     x, y \u001b[39m=\u001b[39m promote_input_dtypes(inputs)\n\u001b[1;32m   1395\u001b[0m     res \u001b[39m=\u001b[39m mb\u001b[39m.\u001b[39mmul(x\u001b[39m=\u001b[39mx, y\u001b[39m=\u001b[39my, name\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/ops.py:188\u001b[0m, in \u001b[0;36m_get_inputs\u001b[0;34m(context, node, expected, min_expected)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_inputs\u001b[39m(context, node, expected\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, min_expected\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Var]:\n\u001b[1;32m    183\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m    Look up a node's inputs in @context and return them as a list. If\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    @expected is not None, also verifies the number of inputs matches the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    value of @expected.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     inputs \u001b[39m=\u001b[39m [context[name] \u001b[39mfor\u001b[39;49;00m name \u001b[39min\u001b[39;49;00m node\u001b[39m.\u001b[39;49minputs]\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m expected \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         expected \u001b[39m=\u001b[39m [expected] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(expected, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39melse\u001b[39;00m expected\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/ops.py:188\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_inputs\u001b[39m(context, node, expected\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, min_expected\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Var]:\n\u001b[1;32m    183\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m    Look up a node's inputs in @context and return them as a list. If\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m    @expected is not None, also verifies the number of inputs matches the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    value of @expected.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     inputs \u001b[39m=\u001b[39m [context[name] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39minputs]\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m expected \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         expected \u001b[39m=\u001b[39m [expected] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(expected, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39melse\u001b[39;00m expected\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/coremltools/converters/mil/frontend/torch/converter.py:86\u001b[0m, in \u001b[0;36mTranscriptionContext.__getitem__\u001b[0;34m(self, torch_name)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m torch_name \u001b[39min\u001b[39;00m current_graph:\n\u001b[1;32m     85\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_graph[idx][torch_name]\n\u001b[0;32m---> 86\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTorch var \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not found in context \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(torch_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m     88\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Torch var alpha.2 not found in context "
     ]
    }
   ],
   "source": [
    "import coremltools as ct \n",
    "\n",
    "coreml_model = ct.convert(\n",
    "    torchtraced_model, convert_to='mlprogram', inputs=[ct.TensorType(shape=(600, 600))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfpgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
